{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with labels have been extracted and saved to 'audio_features_without_filtering.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to audio files\n",
    "audio_folder_path = r'AudioWAV'\n",
    "\n",
    "# Prepare list to hold features and labels\n",
    "features_list = []\n",
    "emotions = []\n",
    "\n",
    "# Define a dictionary for mapping emotion codes to full labels\n",
    "emotion_dict = {\n",
    "    'ANG': 'Anger',\n",
    "    'DIS': 'Disgust',\n",
    "    'FEA': 'Fear',\n",
    "    'HAP': 'Happy',\n",
    "    'NEU': 'Neutral',\n",
    "    'SAD': 'Sad'\n",
    "}\n",
    "\n",
    "# Loop through each audio file in the folder\n",
    "for filename in os.listdir(audio_folder_path):\n",
    "    if filename.endswith(\".wav\"):\n",
    "        file_path = os.path.join(audio_folder_path, filename)\n",
    "        \n",
    "        # Load audio\n",
    "        y, sr = librosa.load(file_path)\n",
    "        \n",
    "        # Define 20ms window size\n",
    "        window_size_ms = 0.02  # 20ms\n",
    "        frame_length = int(window_size_ms * sr)  # Samples for 20ms window\n",
    "        hop_length = frame_length // 2  # 50% overlap\n",
    "        \n",
    "        # Extract MFCC\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, hop_length=hop_length, n_fft=frame_length)\n",
    "        \n",
    "        # Extract delta and delta-delta MFCC\n",
    "        delta_mfcc = librosa.feature.delta(mfcc)\n",
    "        delta2_mfcc = librosa.feature.delta(mfcc, order=2)\n",
    "        \n",
    "        # Calculate statistics for MFCC, delta-MFCC, and delta-delta-MFCC\n",
    "        mfcc_features = np.concatenate([\n",
    "            np.mean(mfcc, axis=1), np.std(mfcc, axis=1)\n",
    "        ])\n",
    "        delta_mfcc_features = np.concatenate([\n",
    "            np.mean(delta_mfcc, axis=1), np.std(delta_mfcc, axis=1)\n",
    "        ])\n",
    "        delta2_mfcc_features = np.concatenate([\n",
    "            np.mean(delta2_mfcc, axis=1), np.std(delta2_mfcc, axis=1)\n",
    "        ])\n",
    "        \n",
    "        # Combine all features\n",
    "        features = np.concatenate([mfcc_features, delta_mfcc_features, delta2_mfcc_features])\n",
    "        features_list.append(features)\n",
    "        \n",
    "        # Extract emotion labels from filename\n",
    "        parts = filename.split('_')\n",
    "        emotion_code = parts[2]  # Emotion code (e.g., ANG, DIS)\n",
    "        \n",
    "        # Map emotion code to full emotion label\n",
    "        emotion_label = emotion_dict.get(emotion_code, \"Unknown\")\n",
    "        emotions.append(emotion_label)\n",
    "\n",
    "# Define column names for DataFrame\n",
    "columns = []\n",
    "columns += [f'mfcc_mean_{i}' for i in range(13)] + [f'mfcc_std_{i}' for i in range(13)]\n",
    "columns += [f'delta_mfcc_mean_{i}' for i in range(13)] + [f'delta_mfcc_std_{i}' for i in range(13)]\n",
    "columns += [f'delta2_mfcc_mean_{i}' for i in range(13)] + [f'delta2_mfcc_std_{i}' for i in range(13)]\n",
    "\n",
    "# Create DataFrame with features and labels\n",
    "df = pd.DataFrame(features_list, columns=columns)\n",
    "df['emotion'] = emotions  # Add emotion labels\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('audio_features_without_filtering.csv', index=False)\n",
    "print(\"Features with labels have been extracted and saved to 'audio_features_without_filtering.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfcc_mean_0</th>\n",
       "      <th>mfcc_mean_1</th>\n",
       "      <th>mfcc_mean_2</th>\n",
       "      <th>mfcc_mean_3</th>\n",
       "      <th>mfcc_mean_4</th>\n",
       "      <th>mfcc_mean_5</th>\n",
       "      <th>mfcc_mean_6</th>\n",
       "      <th>mfcc_mean_7</th>\n",
       "      <th>mfcc_mean_8</th>\n",
       "      <th>mfcc_mean_9</th>\n",
       "      <th>...</th>\n",
       "      <th>delta2_mfcc_std_4</th>\n",
       "      <th>delta2_mfcc_std_5</th>\n",
       "      <th>delta2_mfcc_std_6</th>\n",
       "      <th>delta2_mfcc_std_7</th>\n",
       "      <th>delta2_mfcc_std_8</th>\n",
       "      <th>delta2_mfcc_std_9</th>\n",
       "      <th>delta2_mfcc_std_10</th>\n",
       "      <th>delta2_mfcc_std_11</th>\n",
       "      <th>delta2_mfcc_std_12</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-508.958221</td>\n",
       "      <td>118.634850</td>\n",
       "      <td>-3.356938</td>\n",
       "      <td>40.970490</td>\n",
       "      <td>3.092985</td>\n",
       "      <td>15.026862</td>\n",
       "      <td>-17.768225</td>\n",
       "      <td>-5.002903</td>\n",
       "      <td>-11.964508</td>\n",
       "      <td>-2.992107</td>\n",
       "      <td>...</td>\n",
       "      <td>1.868941</td>\n",
       "      <td>1.050583</td>\n",
       "      <td>1.215996</td>\n",
       "      <td>1.395098</td>\n",
       "      <td>1.106331</td>\n",
       "      <td>0.931807</td>\n",
       "      <td>1.114635</td>\n",
       "      <td>0.931815</td>\n",
       "      <td>1.174189</td>\n",
       "      <td>Anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-556.846436</td>\n",
       "      <td>129.902359</td>\n",
       "      <td>-12.017046</td>\n",
       "      <td>52.574577</td>\n",
       "      <td>5.240597</td>\n",
       "      <td>24.657318</td>\n",
       "      <td>-22.256861</td>\n",
       "      <td>5.648675</td>\n",
       "      <td>-12.966894</td>\n",
       "      <td>-2.986880</td>\n",
       "      <td>...</td>\n",
       "      <td>1.694662</td>\n",
       "      <td>1.328769</td>\n",
       "      <td>1.243354</td>\n",
       "      <td>1.272423</td>\n",
       "      <td>0.964516</td>\n",
       "      <td>0.959910</td>\n",
       "      <td>1.023868</td>\n",
       "      <td>1.036770</td>\n",
       "      <td>1.011040</td>\n",
       "      <td>Disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-512.270203</td>\n",
       "      <td>106.605949</td>\n",
       "      <td>4.481588</td>\n",
       "      <td>33.221615</td>\n",
       "      <td>10.252079</td>\n",
       "      <td>11.566597</td>\n",
       "      <td>-15.322458</td>\n",
       "      <td>-2.085884</td>\n",
       "      <td>-8.047853</td>\n",
       "      <td>-5.309411</td>\n",
       "      <td>...</td>\n",
       "      <td>1.737946</td>\n",
       "      <td>1.255807</td>\n",
       "      <td>1.169335</td>\n",
       "      <td>1.337092</td>\n",
       "      <td>0.984128</td>\n",
       "      <td>0.957654</td>\n",
       "      <td>0.902779</td>\n",
       "      <td>0.929083</td>\n",
       "      <td>0.755707</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-507.348145</td>\n",
       "      <td>120.728325</td>\n",
       "      <td>-9.146874</td>\n",
       "      <td>42.703300</td>\n",
       "      <td>7.847485</td>\n",
       "      <td>13.112681</td>\n",
       "      <td>-21.243057</td>\n",
       "      <td>-0.663292</td>\n",
       "      <td>-12.059789</td>\n",
       "      <td>-8.139682</td>\n",
       "      <td>...</td>\n",
       "      <td>1.628116</td>\n",
       "      <td>1.237468</td>\n",
       "      <td>1.284604</td>\n",
       "      <td>1.347286</td>\n",
       "      <td>1.266892</td>\n",
       "      <td>1.093934</td>\n",
       "      <td>1.267915</td>\n",
       "      <td>0.997715</td>\n",
       "      <td>1.104413</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-538.608521</td>\n",
       "      <td>123.066727</td>\n",
       "      <td>-0.616907</td>\n",
       "      <td>40.799046</td>\n",
       "      <td>10.415956</td>\n",
       "      <td>14.509365</td>\n",
       "      <td>-13.737962</td>\n",
       "      <td>-3.017844</td>\n",
       "      <td>-11.575838</td>\n",
       "      <td>-6.111917</td>\n",
       "      <td>...</td>\n",
       "      <td>1.529162</td>\n",
       "      <td>1.156719</td>\n",
       "      <td>1.291078</td>\n",
       "      <td>1.373941</td>\n",
       "      <td>1.104398</td>\n",
       "      <td>1.017255</td>\n",
       "      <td>1.232664</td>\n",
       "      <td>1.270499</td>\n",
       "      <td>0.978076</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7437</th>\n",
       "      <td>-622.698975</td>\n",
       "      <td>127.347740</td>\n",
       "      <td>16.230566</td>\n",
       "      <td>54.328545</td>\n",
       "      <td>-11.143003</td>\n",
       "      <td>42.037441</td>\n",
       "      <td>-17.240387</td>\n",
       "      <td>12.382068</td>\n",
       "      <td>-12.402145</td>\n",
       "      <td>5.875704</td>\n",
       "      <td>...</td>\n",
       "      <td>1.516225</td>\n",
       "      <td>1.382194</td>\n",
       "      <td>1.144804</td>\n",
       "      <td>1.143540</td>\n",
       "      <td>1.030847</td>\n",
       "      <td>0.981285</td>\n",
       "      <td>0.950538</td>\n",
       "      <td>0.969847</td>\n",
       "      <td>1.156949</td>\n",
       "      <td>Disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7438</th>\n",
       "      <td>-633.114319</td>\n",
       "      <td>123.366188</td>\n",
       "      <td>16.944687</td>\n",
       "      <td>59.779652</td>\n",
       "      <td>-8.737967</td>\n",
       "      <td>43.901775</td>\n",
       "      <td>-17.874489</td>\n",
       "      <td>11.103239</td>\n",
       "      <td>-9.550373</td>\n",
       "      <td>5.951777</td>\n",
       "      <td>...</td>\n",
       "      <td>1.328429</td>\n",
       "      <td>1.144373</td>\n",
       "      <td>1.054839</td>\n",
       "      <td>1.203048</td>\n",
       "      <td>0.943827</td>\n",
       "      <td>0.941787</td>\n",
       "      <td>1.028106</td>\n",
       "      <td>0.868207</td>\n",
       "      <td>1.089416</td>\n",
       "      <td>Fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7439</th>\n",
       "      <td>-581.527649</td>\n",
       "      <td>122.734200</td>\n",
       "      <td>12.707020</td>\n",
       "      <td>53.749996</td>\n",
       "      <td>-9.003664</td>\n",
       "      <td>29.595524</td>\n",
       "      <td>-20.057116</td>\n",
       "      <td>9.435844</td>\n",
       "      <td>-12.970961</td>\n",
       "      <td>5.856920</td>\n",
       "      <td>...</td>\n",
       "      <td>1.310597</td>\n",
       "      <td>1.508141</td>\n",
       "      <td>1.530938</td>\n",
       "      <td>1.222889</td>\n",
       "      <td>1.369629</td>\n",
       "      <td>1.065622</td>\n",
       "      <td>1.064884</td>\n",
       "      <td>0.956855</td>\n",
       "      <td>0.909332</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7440</th>\n",
       "      <td>-598.897339</td>\n",
       "      <td>119.986755</td>\n",
       "      <td>23.080889</td>\n",
       "      <td>45.074135</td>\n",
       "      <td>-11.070361</td>\n",
       "      <td>30.927126</td>\n",
       "      <td>-11.557021</td>\n",
       "      <td>7.765401</td>\n",
       "      <td>-10.457713</td>\n",
       "      <td>6.325374</td>\n",
       "      <td>...</td>\n",
       "      <td>1.805060</td>\n",
       "      <td>1.358548</td>\n",
       "      <td>1.153678</td>\n",
       "      <td>1.357628</td>\n",
       "      <td>1.106659</td>\n",
       "      <td>0.998317</td>\n",
       "      <td>1.261371</td>\n",
       "      <td>0.988496</td>\n",
       "      <td>1.027129</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7441</th>\n",
       "      <td>-633.450134</td>\n",
       "      <td>125.347176</td>\n",
       "      <td>18.022074</td>\n",
       "      <td>52.824436</td>\n",
       "      <td>-8.318122</td>\n",
       "      <td>39.511272</td>\n",
       "      <td>-14.978090</td>\n",
       "      <td>11.737441</td>\n",
       "      <td>-9.941620</td>\n",
       "      <td>7.231276</td>\n",
       "      <td>...</td>\n",
       "      <td>1.246522</td>\n",
       "      <td>1.173530</td>\n",
       "      <td>1.051340</td>\n",
       "      <td>1.185447</td>\n",
       "      <td>0.998718</td>\n",
       "      <td>1.061677</td>\n",
       "      <td>1.060615</td>\n",
       "      <td>1.014752</td>\n",
       "      <td>1.173886</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7442 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mfcc_mean_0  mfcc_mean_1  mfcc_mean_2  mfcc_mean_3  mfcc_mean_4  \\\n",
       "0     -508.958221   118.634850    -3.356938    40.970490     3.092985   \n",
       "1     -556.846436   129.902359   -12.017046    52.574577     5.240597   \n",
       "2     -512.270203   106.605949     4.481588    33.221615    10.252079   \n",
       "3     -507.348145   120.728325    -9.146874    42.703300     7.847485   \n",
       "4     -538.608521   123.066727    -0.616907    40.799046    10.415956   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "7437  -622.698975   127.347740    16.230566    54.328545   -11.143003   \n",
       "7438  -633.114319   123.366188    16.944687    59.779652    -8.737967   \n",
       "7439  -581.527649   122.734200    12.707020    53.749996    -9.003664   \n",
       "7440  -598.897339   119.986755    23.080889    45.074135   -11.070361   \n",
       "7441  -633.450134   125.347176    18.022074    52.824436    -8.318122   \n",
       "\n",
       "      mfcc_mean_5  mfcc_mean_6  mfcc_mean_7  mfcc_mean_8  mfcc_mean_9  ...  \\\n",
       "0       15.026862   -17.768225    -5.002903   -11.964508    -2.992107  ...   \n",
       "1       24.657318   -22.256861     5.648675   -12.966894    -2.986880  ...   \n",
       "2       11.566597   -15.322458    -2.085884    -8.047853    -5.309411  ...   \n",
       "3       13.112681   -21.243057    -0.663292   -12.059789    -8.139682  ...   \n",
       "4       14.509365   -13.737962    -3.017844   -11.575838    -6.111917  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "7437    42.037441   -17.240387    12.382068   -12.402145     5.875704  ...   \n",
       "7438    43.901775   -17.874489    11.103239    -9.550373     5.951777  ...   \n",
       "7439    29.595524   -20.057116     9.435844   -12.970961     5.856920  ...   \n",
       "7440    30.927126   -11.557021     7.765401   -10.457713     6.325374  ...   \n",
       "7441    39.511272   -14.978090    11.737441    -9.941620     7.231276  ...   \n",
       "\n",
       "      delta2_mfcc_std_4  delta2_mfcc_std_5  delta2_mfcc_std_6  \\\n",
       "0              1.868941           1.050583           1.215996   \n",
       "1              1.694662           1.328769           1.243354   \n",
       "2              1.737946           1.255807           1.169335   \n",
       "3              1.628116           1.237468           1.284604   \n",
       "4              1.529162           1.156719           1.291078   \n",
       "...                 ...                ...                ...   \n",
       "7437           1.516225           1.382194           1.144804   \n",
       "7438           1.328429           1.144373           1.054839   \n",
       "7439           1.310597           1.508141           1.530938   \n",
       "7440           1.805060           1.358548           1.153678   \n",
       "7441           1.246522           1.173530           1.051340   \n",
       "\n",
       "      delta2_mfcc_std_7  delta2_mfcc_std_8  delta2_mfcc_std_9  \\\n",
       "0              1.395098           1.106331           0.931807   \n",
       "1              1.272423           0.964516           0.959910   \n",
       "2              1.337092           0.984128           0.957654   \n",
       "3              1.347286           1.266892           1.093934   \n",
       "4              1.373941           1.104398           1.017255   \n",
       "...                 ...                ...                ...   \n",
       "7437           1.143540           1.030847           0.981285   \n",
       "7438           1.203048           0.943827           0.941787   \n",
       "7439           1.222889           1.369629           1.065622   \n",
       "7440           1.357628           1.106659           0.998317   \n",
       "7441           1.185447           0.998718           1.061677   \n",
       "\n",
       "      delta2_mfcc_std_10  delta2_mfcc_std_11  delta2_mfcc_std_12  emotion  \n",
       "0               1.114635            0.931815            1.174189    Anger  \n",
       "1               1.023868            1.036770            1.011040  Disgust  \n",
       "2               0.902779            0.929083            0.755707     Fear  \n",
       "3               1.267915            0.997715            1.104413    Happy  \n",
       "4               1.232664            1.270499            0.978076  Neutral  \n",
       "...                  ...                 ...                 ...      ...  \n",
       "7437            0.950538            0.969847            1.156949  Disgust  \n",
       "7438            1.028106            0.868207            1.089416     Fear  \n",
       "7439            1.064884            0.956855            0.909332    Happy  \n",
       "7440            1.261371            0.988496            1.027129  Neutral  \n",
       "7441            1.060615            1.014752            1.173886      Sad  \n",
       "\n",
       "[7442 rows x 79 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test data have been saved to 'training_data.csv' and 'test_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data from CSV file\n",
    "file_path = 'C:/Users/eswar/Desktop/Ml HW/Final Project/audio_features_without_filtering.csv'  # Replace with your local file path\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define features (X) and target (y) columns\n",
    "X = data.drop(columns=['emotion'])  # Drop the target column to get features\n",
    "y = data['emotion']  # Define the target column\n",
    "\n",
    "# Split the data into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Combine X and y for each set and save them back to CSV\n",
    "train_data = pd.concat([X_train, y_train], axis=1)\n",
    "test_data = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "# Save to new CSV files\n",
    "train_data.to_csv('train.csv', index=False)\n",
    "test_data.to_csv('test.csv', index=False)\n",
    "\n",
    "print(\"Training and test data have been saved to 'training_data.csv' and 'test_data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA transformation complete.\n",
      "Training features shape after PCA: (5953, 50)\n",
      "Testing features shape after PCA: (1489, 50)\n",
      "Explained variance by selected components: 0.9530030097876494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the train and test CSV files with extracted features\n",
    "train_df = pd.read_csv('C:/Users/eswar/Desktop/Ml HW/Final Project/train.csv')\n",
    "test_df = pd.read_csv('C:/Users/eswar/Desktop/Ml HW/Final Project/test.csv')\n",
    "\n",
    "# Drop the 'intensity' column if it exists\n",
    "if 'intensity' in train_df.columns:\n",
    "    train_df = train_df.drop(columns=['intensity'])\n",
    "\n",
    "if 'intensity' in test_df.columns:\n",
    "    test_df = test_df.drop(columns=['intensity'])\n",
    "\n",
    "# Separate features and labels, assuming 'emotion' is the target label\n",
    "if 'emotion' in train_df.columns:\n",
    "    y_train = train_df['emotion'].reset_index(drop=True)\n",
    "    train_features = train_df.drop(columns=['emotion']).reset_index(drop=True)\n",
    "else:\n",
    "    train_features = train_df.reset_index(drop=True)\n",
    "\n",
    "if 'emotion' in test_df.columns:\n",
    "    y_test = test_df['emotion'].reset_index(drop=True)\n",
    "    test_features = test_df.drop(columns=['emotion']).reset_index(drop=True)\n",
    "else:\n",
    "    test_features = test_df.reset_index(drop=True)\n",
    "\n",
    "# Impute missing values with the mean of each column\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "train_features = pd.DataFrame(imputer.fit_transform(train_features), columns=train_features.columns)\n",
    "test_features = pd.DataFrame(imputer.transform(test_features), columns=test_features.columns)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "train_features = pd.DataFrame(scaler.fit_transform(train_features), columns=train_features.columns)\n",
    "test_features = pd.DataFrame(scaler.transform(test_features), columns=test_features.columns)\n",
    "\n",
    "# Apply PCA to retain 95% of the explained variance\n",
    "pca = PCA(n_components=0.95)\n",
    "train_features_pca = pca.fit_transform(train_features)\n",
    "test_features_pca = pca.transform(test_features)\n",
    "\n",
    "# Convert PCA-transformed features back to DataFrame and reset index\n",
    "train_features_pca_df = pd.DataFrame(train_features_pca).reset_index(drop=True)\n",
    "test_features_pca_df = pd.DataFrame(test_features_pca).reset_index(drop=True)\n",
    "\n",
    "# Add 'emotion' labels back to the transformed data if they were present\n",
    "if 'emotion' in train_df.columns:\n",
    "    train_features_pca_df['emotion'] = y_train\n",
    "    test_features_pca_df['emotion'] = y_test\n",
    "\n",
    "# Save PCA-transformed data back to CSV\n",
    "train_features_pca_df.to_csv('train_pca.csv', index=False)\n",
    "test_features_pca_df.to_csv('test_pca.csv', index=False)\n",
    "\n",
    "print(\"PCA transformation complete.\")\n",
    "print(\"Training features shape after PCA:\", train_features_pca_df.shape)\n",
    "print(\"Testing features shape after PCA:\", test_features_pca_df.shape)\n",
    "print(\"Explained variance by selected components:\", np.sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores (svm): [0.48026868 0.50461797 0.49622166 0.4789916  0.50672269]\n",
      "Mean Cross-Validation Score (svm): 0.49336451961137107\n",
      "Predictions saved to 'predict_svm.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load PCA-transformed train and test data\n",
    "X_train = pd.read_csv(r'C:/Users/eswar/Desktop/Ml HW/Final Project/train_pca.csv')\n",
    "X_test = pd.read_csv(r'C:/Users/eswar/Desktop/Ml HW/Final Project/test_pca.csv')\n",
    "\n",
    "# Separate target labels from features in training data\n",
    "Y_train = X_train['emotion']\n",
    "X_train = X_train.drop(columns=['emotion'], errors='ignore')  # Ensure only features in X_train\n",
    "X_test = X_test.drop(columns=['emotion'], errors='ignore')    # Ensure only features in X_test\n",
    "\n",
    "# Function to perform cross-validation\n",
    "def cross_validate(X_train, Y_train, model_type=\"svm\"):\n",
    "    model = SVC() if model_type == \"svm\" else RandomForestClassifier(n_estimators=800)\n",
    "    scores = cross_val_score(model, X_train, Y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"Cross-Validation Scores ({model_type}):\", scores)\n",
    "    print(f\"Mean Cross-Validation Score ({model_type}): {scores.mean()}\")\n",
    "\n",
    "# SVM classifier function\n",
    "def svm_classifier(X_train, Y_train, X_test):\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, Y_train)\n",
    "    return svm.predict(X_test)\n",
    "\n",
    "# Perform cross-validation on PCA features\n",
    "cross_validate(X_train, Y_train, model_type=\"svm\")\n",
    "\n",
    "# Classify PCA test features using the SVM classifier\n",
    "y_test_svm = svm_classifier(X_train, Y_train, X_test)\n",
    "\n",
    "# Save predictions\n",
    "test_df = pd.DataFrame(X_test)  # Initialize with X_test to get correct row count\n",
    "test_df['emotion'] = y_test_svm  # Add predictions\n",
    "test_df.to_csv(r'C:/Users/eswar/Desktop/Ml HW/Final Project/predict_svm.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'predict_svm.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
